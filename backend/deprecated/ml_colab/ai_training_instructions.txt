â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    AI MODEL TRAINING INSTRUCTIONS
    Turkish Pronunciation Analysis System
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

This document provides step-by-step instructions for training the
pronunciation quality assessment model.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ¯ OVERVIEW
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Purpose: Train a deep learning model to assess Turkish pronunciation
         quality based on acoustic features.

Architecture: Feed-forward neural network with:
              - Input: 35-dimensional feature vectors
              - Hidden layers: [128, 64, 32] neurons
              - Output: Binary classification (good/needs improvement)

Training Platform: Google Colab with GPU acceleration

Expected Duration: ~20-30 minutes


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“‹ PRE-TRAINING CHECKLIST
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Before starting training, ensure:

â˜ Data Collection Complete
  - Minimum 50 participants recommended
  - Each participant recorded 30 words
  - Total: 1500+ audio samples

â˜ Data Quality Verified
  - Audio files are valid .wav format
  - Sample rate: 16kHz (will be resampled if different)
  - No silent or corrupted recordings

â˜ Labels Prepared
  âš ï¸ CRITICAL: You must provide pronunciation quality labels!
  
â˜ Phoneme Generation (Optional)
  - Use the phoneme API to generate reference phoneme sequences
  - Endpoint: POST /phoneme/generate or /phoneme/batch
  - Useful for phoneme-level alignment and distance calculations
  
  Options for labeling:
  a) Expert Annotations: Phoneticians rate each recording
  b) Native Speaker Ratings: 1-5 Likert scale
  c) Phonetic Distance: Compare to reference recordings
  d) Automated IPA Transcription: Use phonemizer + DTW
  
  The notebook includes SYNTHETIC labels as placeholder.
  Replace the generate_synthetic_labels() function with real data!

â˜ Google Drive Setup
  - Data uploaded to Drive
  - Path: MyDrive/phoneizer/data/
  - Sufficient storage space (recommend 5GB+)

â˜ Colab Access
  - GPU runtime enabled
  - Drive mounting permissions granted


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸš‚ TRAINING WORKFLOW
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PHASE 1: Environment Setup (5 minutes)
---------------------------------------
1. Open training_notebook.ipynb in Google Colab
2. Enable GPU: Runtime â†’ Change runtime type â†’ GPU
3. Run setup cells to install dependencies
4. Mount Google Drive when prompted

Expected output:
âœ“ PyTorch installed with CUDA support
âœ“ Drive mounted at /content/drive
âœ“ Using device: cuda


PHASE 2: Data Loading (2-5 minutes)
------------------------------------
1. Execute data loading cells
2. Notebook will scan all participant directories
3. Load audio files and metadata

Expected output:
Loaded XXX audio samples from YY participants

If count is too low:
- Check Drive path is correct
- Verify folder structure matches expected format


PHASE 3: Feature Extraction (10-20 minutes)
--------------------------------------------
â±ï¸ This is the longest phase!

For each audio file, extracts:
- 13 MFCCs (mean + std) = 26 features
- Spectral features (centroid, rolloff, bandwidth) = 3
- Zero crossing rate = 1
- RMS energy = 1
- Formants (F1, F2, F3) = 3
- F0 statistics (mean, std) = 2
- Duration = 1
  
Total: 37 acoustic features

Progress bar will show completion status.

Expected output:
Feature extraction complete. XXX samples remaining.

If errors occur:
- Check audio file integrity
- Some files may fail (acceptable if <5%)
- Failed files are automatically dropped


PHASE 4: Label Assignment
--------------------------
âš ï¸ REPLACE SYNTHETIC LABELS WITH REAL DATA

The notebook includes:
```python
def generate_synthetic_labels(features):
    quality_score = np.random.uniform(0.3, 1.0)
    return quality_score
```

Replace this with your actual labeling logic!

Example with manual annotations:
```python
# Load annotations CSV
annotations = pd.read_csv('annotations.csv')
labels_dict = dict(zip(annotations['audio_path'], 
                       annotations['quality_score']))

df['quality_score'] = df['audio_path'].map(labels_dict)
```


PHASE 5: Training (15-25 minutes)
----------------------------------
Model will train for 50 epochs.

Monitor:
- Train Loss: Should decrease steadily
- Test Loss: Should track train loss
- Accuracy: Should increase to 70-85%

Good training signs:
âœ“ Losses decrease smoothly
âœ“ Test loss follows train loss (no overfitting)
âœ“ Accuracy improves over time

Warning signs:
âš ï¸ Train loss much lower than test loss â†’ Overfitting
   Solution: Increase dropout, reduce model size
   
âš ï¸ Loss plateaus early â†’ Learning rate too high/low
   Solution: Adjust learning rate (try 0.0001 or 0.01)
   
âš ï¸ Accuracy stuck at 50% â†’ Model not learning
   Solution: Check labels, increase model capacity

Best model is automatically saved when test loss improves.


PHASE 6: Evaluation
--------------------
Review training curves and final metrics.

Expected accuracy range:
- 70-80%: Good, ready for production
- 80-90%: Excellent
- <70%: May need more data or better labels
- >95%: Suspicious, check for data leakage


PHASE 7: Model Export
----------------------
1. Model saved to: MyDrive/phoneizer/models/trained_model.pt
2. Scaler saved to: MyDrive/phoneizer/models/scaler.pkl

File sizes:
- trained_model.pt: ~500KB - 2MB
- scaler.pkl: <10KB

Download both files to your local machine.


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ’¾ POST-TRAINING DEPLOYMENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

STEP 1: Copy Model to Backend
------------------------------
Copy downloaded files to:
  backend/../models/trained_model.pt
  backend/../models/scaler.pkl


STEP 2: Verify Backend Integration
-----------------------------------
Start backend server:
  cd backend
  python main.py

Check logs for:
  âœ“ Model loaded successfully

If you see:
  âš  Model not found
  â†’ Check file path is correct


STEP 3: Test API Endpoint
--------------------------
Upload a test audio file through the frontend.
Backend should return:
{
  "word": "test",
  "score": 0.75,
  "confidence": 0.85,
  "feedback": "Ä°yi! KÃ¼Ã§Ã¼k iyileÅŸtirmeler yapabilirsiniz."
}


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”„ RETRAINING WORKFLOW
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

When to retrain:
- New participant data collected (every 50-100 samples)
- Model accuracy degrades
- Feedback from users indicates poor predictions
- Adding new words to vocabulary

Retraining process:
1. Collect new data through web interface
2. Upload updated /data folder to Drive
3. Re-run notebook from PHASE 2 onwards
4. Compare new model metrics to old model
5. Deploy if metrics improve


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“Š HYPERPARAMETER TUNING GUIDE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

If model performance is suboptimal, try adjusting:

Architecture:
  hidden_sizes = [256, 128, 64]  # More capacity
  hidden_sizes = [64, 32]        # Less capacity (prevent overfitting)

Regularization:
  dropout = 0.5      # More regularization
  dropout = 0.1      # Less regularization

Learning Rate:
  lr = 0.0001       # Slower, more stable
  lr = 0.01         # Faster, may be unstable

Training:
  batch_size = 16   # Smaller batches, noisier gradients
  batch_size = 64   # Larger batches, smoother gradients
  
  num_epochs = 100  # More training time

Always compare validation metrics between runs!


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âš ï¸ COMMON ISSUES & SOLUTIONS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Issue: "CUDA out of memory"
Solution:
  - Reduce batch_size to 16 or 8
  - Restart Colab runtime
  - Use smaller model architecture

Issue: "Permission denied" when mounting Drive
Solution:
  - Re-run mount cell
  - Grant all permissions in popup
  - Check Google account is logged in

Issue: Training is extremely slow
Solution:
  - Verify GPU is enabled (not CPU)
  - Check Runtime â†’ Change runtime type â†’ GPU
  - Reduce dataset size for testing

Issue: Model accuracy is 50% (random guessing)
Solution:
  - Check label quality
  - Verify label distribution (not all 0s or 1s)
  - Increase model capacity
  - Train for more epochs

Issue: Perfect accuracy (100%)
Solution:
  - Data leakage! Check train/test split
  - Verify labels are not derived from features
  - Ensure no duplicate samples

Issue: Model file not loading in backend
Solution:
  - Check PyTorch versions match (2.1.2)
  - Use torch.load() with map_location='cpu'
  - Verify file downloaded completely (check size)


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ˆ MONITORING & LOGGING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Track these metrics across training runs:

Training Metrics:
- Final train loss
- Final test loss  
- Best test accuracy
- Training time

Model Metadata:
- Date trained
- Number of samples
- Participant count
- Feature dimensions
- Hyperparameters used

Production Metrics (after deployment):
- Inference time per request
- User feedback on predictions
- API error rate


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”¤ USING PHONEME API IN TRAINING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

The PhoneticHybrid backend now includes a phoneme generation API
that can be used during the training phase for improved labeling.

API Endpoints:
--------------
1. /phoneme/generate - Generate phonemes for single word
2. /phoneme/analyze  - Detailed analysis with syllable count
3. /phoneme/batch    - Process multiple words at once
4. /phoneme/health   - Check service availability

Usage in Colab Notebook:
------------------------
```python
import requests

# Generate phonemes for a word
response = requests.post(
    'http://localhost:8000/phoneme/generate',
    json={'word': 'pencere', 'include_stress': True}
)
data = response.json()
print(f"Word: {data['word']}")
print(f"Phonemes: {data['phonemes']}")
print(f"Count: {data['phoneme_count']}")

# Batch processing for all words
words = ['araba', 'bahÃ§e', 'Ã§ocuk', 'diÅŸ', 'elma']
response = requests.post(
    'http://localhost:8000/phoneme/batch',
    json=words
)
phoneme_data = response.json()
```

Integration with Training Pipeline:
------------------------------------
Use phoneme sequences to:
1. Generate reference phoneme targets for each word
2. Calculate phoneme edit distance as a feature
3. Align recorded audio phonemes with reference
4. Create phoneme-level pronunciation accuracy scores

Example Feature:
```python
def calculate_phoneme_distance(word, audio_path):
    # Get reference phonemes
    ref_phonemes = get_phonemes_from_api(word)
    
    # Extract phonemes from audio (using forced alignment)
    audio_phonemes = extract_phonemes_from_audio(audio_path)
    
    # Calculate edit distance
    distance = levenshtein_distance(ref_phonemes, audio_phonemes)
    
    return distance
```

This phoneme-based approach can significantly improve
pronunciation quality assessment accuracy.


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ ADVANCED: FEATURE ENGINEERING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

To improve model performance, consider adding:

Temporal Features:
- MFCC deltas (first derivative)
- MFCC delta-deltas (second derivative)
- Tempo and rhythm features

Linguistic Features:
- Phoneme-level alignment (use /phoneme API)
- IPA transcription matching
- Edit distance to reference

Prosodic Features:
- Intonation contours
- Stress patterns
- Speech rate

Implementation:
Modify extract_acoustic_features() function in notebook.


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… TRAINING COMPLETION CHECKLIST
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Before deploying, verify:

â˜ Model trained successfully
â˜ Test accuracy â‰¥ 70%
â˜ No overfitting (train/test loss similar)
â˜ Model files downloaded from Drive
â˜ Files copied to backend/models/
â˜ Backend loads model without errors
â˜ Test API endpoint returns predictions
â˜ Predictions are reasonable (not random)
â˜ Training notebook saved for future reference
â˜ Training metrics documented


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
END OF TRAINING INSTRUCTIONS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
