{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turkish Pronunciation Analysis - Model Training\n",
    "### PhoneticHybrid ML Training Pipeline\n",
    "\n",
    "This notebook trains a deep learning model for Turkish pronunciation quality assessment.\n",
    "\n",
    "**Architecture:** CNN/RNN hybrid for acoustic feature classification\n",
    "\n",
    "**Features:**\n",
    "- MFCCs (Mel-frequency cepstral coefficients)\n",
    "- Formants (F1, F2, F3)\n",
    "- Fundamental frequency (F0)\n",
    "- RMS energy\n",
    "- Duration features\n",
    "\n",
    "**Training Environment:** Google Colab with GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install librosa soundfile\n",
    "!pip install praat-parselmouth\n",
    "!pip install phonemizer\n",
    "!pip install scikit-learn pandas numpy matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set paths\n",
    "DATA_DIR = '/content/drive/MyDrive/phoneizer/data'\n",
    "MODELS_DIR = '/content/drive/MyDrive/phoneizer/models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_audio_files(data_dir):\n",
    "    \"\"\"Load all audio files and metadata from participant directories.\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for participant_dir in os.listdir(data_dir):\n",
    "        if not participant_dir.startswith('participant_'):\n",
    "            continue\n",
    "            \n",
    "        participant_path = os.path.join(data_dir, participant_dir)\n",
    "        words_dir = os.path.join(participant_path, 'kelimeler')\n",
    "        \n",
    "        if not os.path.exists(words_dir):\n",
    "            continue\n",
    "            \n",
    "        # Load participant info\n",
    "        info_path = os.path.join(participant_path, 'info.json')\n",
    "        with open(info_path, 'r', encoding='utf-8') as f:\n",
    "            participant_info = json.load(f)\n",
    "        \n",
    "        # Load audio files\n",
    "        for audio_file in os.listdir(words_dir):\n",
    "            if audio_file.endswith('.wav'):\n",
    "                audio_path = os.path.join(words_dir, audio_file)\n",
    "                word = audio_file.split('_')[1].replace('.wav', '')\n",
    "                \n",
    "                data.append({\n",
    "                    'participant_id': participant_info['id'],\n",
    "                    'audio_path': audio_path,\n",
    "                    'word': word,\n",
    "                    'age': participant_info['age'],\n",
    "                    'gender': participant_info['gender']\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = load_audio_files(DATA_DIR)\n",
    "print(f'Loaded {len(df)} audio samples from {df.participant_id.nunique()} participants')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_acoustic_features(audio_path, sr=16000):\n",
    "    \"\"\"Extract comprehensive acoustic features from audio file.\"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        y, sr = librosa.load(audio_path, sr=sr)\n",
    "        \n",
    "        # 1. MFCCs (13 coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfcc_mean = np.mean(mfccs, axis=1)\n",
    "        mfcc_std = np.std(mfccs, axis=1)\n",
    "        \n",
    "        # 2. Spectral features\n",
    "        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
    "        spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
    "        \n",
    "        # 3. Zero crossing rate\n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(y))\n",
    "        \n",
    "        # 4. RMS energy\n",
    "        rms = np.mean(librosa.feature.rms(y=y))\n",
    "        \n",
    "        # 5. Formants using Praat\n",
    "        sound = parselmouth.Sound(audio_path)\n",
    "        formant = call(sound, \"To Formant (burg)\", 0.0, 5, 5500, 0.025, 50)\n",
    "        f1 = call(formant, \"Get mean\", 1, 0, 0, \"hertz\")\n",
    "        f2 = call(formant, \"Get mean\", 2, 0, 0, \"hertz\")\n",
    "        f3 = call(formant, \"Get mean\", 3, 0, 0, \"hertz\")\n",
    "        \n",
    "        # 6. Fundamental frequency (F0)\n",
    "        pitch = call(sound, \"To Pitch\", 0.0, 75, 600)\n",
    "        f0_mean = call(pitch, \"Get mean\", 0, 0, \"hertz\")\n",
    "        f0_std = call(pitch, \"Get standard deviation\", 0, 0, \"hertz\")\n",
    "        \n",
    "        # 7. Duration\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        \n",
    "        # Combine all features\n",
    "        features = np.concatenate([\n",
    "            mfcc_mean,\n",
    "            mfcc_std,\n",
    "            [spectral_centroid, spectral_rolloff, spectral_bandwidth],\n",
    "            [zcr, rms],\n",
    "            [f1, f2, f3],\n",
    "            [f0_mean, f0_std],\n",
    "            [duration]\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract features for all samples\n",
    "print('Extracting features...')\n",
    "features_list = []\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    features = extract_acoustic_features(row['audio_path'])\n",
    "    features_list.append(features)\n",
    "\n",
    "df['features'] = features_list\n",
    "df = df.dropna(subset=['features'])\n",
    "print(f'Feature extraction complete. {len(df)} samples remaining.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Label Generation\n",
    "\n",
    "For supervised learning, we need labels. In a real scenario, you would have:\n",
    "- Expert annotations\n",
    "- Reference recordings\n",
    "- Phonetic transcriptions\n",
    "\n",
    "For now, we'll create synthetic labels based on feature quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace with actual labels from expert annotations\n",
    "# For demonstration, create synthetic labels\n",
    "def generate_synthetic_labels(features):\n",
    "    \"\"\"Generate synthetic quality scores for demonstration.\"\"\"\n",
    "    # This is a placeholder - replace with actual labels\n",
    "    quality_score = np.random.uniform(0.3, 1.0)\n",
    "    return quality_score\n",
    "\n",
    "df['quality_score'] = df['features'].apply(generate_synthetic_labels)\n",
    "df['label'] = (df['quality_score'] > 0.6).astype(int)  # Binary classification\n",
    "\n",
    "print('Label distribution:')\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = np.vstack(df['features'].values)\n",
    "y = df['label'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.FloatTensor(X_train)\n",
    "y_train_t = torch.FloatTensor(y_train)\n",
    "X_test_t = torch.FloatTensor(X_test)\n",
    "y_test_t = torch.FloatTensor(y_test)\n",
    "\n",
    "print(f'Training set: {X_train.shape}')\n",
    "print(f'Test set: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Dataset and DataLoader\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = AudioDataset(X_train_t, y_train_t)\n",
    "test_dataset = AudioDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PronunciationQualityNet(nn.Module):\n",
    "    \"\"\"Deep neural network for pronunciation quality assessment.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes=[128, 64, 32], dropout=0.3):\n",
    "        super(PronunciationQualityNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train.shape[1]\n",
    "model = PronunciationQualityNet(input_size).to(device)\n",
    "\n",
    "print(model)\n",
    "print(f'\\nTotal parameters: {sum(p.numel() for p in model.parameters())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)\n",
    "\n",
    "num_epochs = 50\n",
    "best_loss = float('inf')\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for features, labels in train_loader:\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    scheduler.step(test_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] '\n",
    "          f'Train Loss: {train_loss:.4f} | '\n",
    "          f'Test Loss: {test_loss:.4f} | '\n",
    "          f'Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    # Save best model\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        torch.save(model.state_dict(), os.path.join(MODELS_DIR, 'best_model.pt'))\n",
    "        print('✓ Model saved!')\n",
    "\n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y_train, bins=20, alpha=0.5, label='Train')\n",
    "plt.hist(y_test, bins=20, alpha=0.5, label='Test')\n",
    "plt.xlabel('Quality Score')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Label Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODELS_DIR, 'training_curves.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete model\n",
    "torch.save(model, os.path.join(MODELS_DIR, 'trained_model.pt'))\n",
    "\n",
    "# Save scaler\n",
    "import pickle\n",
    "with open(os.path.join(MODELS_DIR, 'scaler.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print('✓ Model and scaler saved successfully!')\n",
    "print(f'Model path: {os.path.join(MODELS_DIR, \"trained_model.pt\")}')\n",
    "print('\\nNow copy this model to your local backend/models/ directory.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
