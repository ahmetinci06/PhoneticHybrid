{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé§ Pronunciation Scoring Model Training\n",
        "\n",
        "Train a neural network to predict pronunciation quality scores (0-100) from acoustic features.\n",
        "\n",
        "**Pipeline:**\n",
        "1. Load prepared dataset (CSV)\n",
        "2. Preprocess & normalize features\n",
        "3. Train neural network model\n",
        "4. Evaluate performance\n",
        "5. Export model for backend integration\n",
        "\n",
        "**Requirements:**\n",
        "- Upload `training_data.csv` from your data preparation\n",
        "- Run on GPU for faster training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch torchvision pandas numpy scikit-learn matplotlib seaborn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Upload Training Data\n",
        "\n",
        "Upload the `training_data.csv` file created by `prepare_training_data.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Upload your training_data.csv file:\")\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Load and Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv('training_data.csv')\n",
        "\n",
        "print(\"Dataset Info:\")\n",
        "print(f\"  Total samples: {len(df)}\")\n",
        "print(f\"  Features: {len(df.columns) - 4}\")\n",
        "print(f\"  Unique words: {df['word'].nunique()}\")\n",
        "print(f\"  Unique participants: {df['participant_id'].nunique()}\")\n",
        "print(f\"\\nScore Statistics:\")\n",
        "print(df['score'].describe())\n",
        "\n",
        "# Display first few rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize score distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(df['score'], bins=20, edgecolor='black')\n",
        "plt.xlabel('Pronunciation Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Score Distribution')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(y=df['score'])\n",
        "plt.ylabel('Pronunciation Score')\n",
        "plt.title('Score Box Plot')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and labels\n",
        "metadata_cols = ['word', 'participant_id', 'audio_file', 'score']\n",
        "feature_cols = [col for col in df.columns if col not in metadata_cols]\n",
        "\n",
        "X = df[feature_cols].values\n",
        "y = df['score'].values\n",
        "\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Label vector shape: {y.shape}\")\n",
        "\n",
        "# Normalize scores to [0, 1] range for training\n",
        "y_normalized = y / 100.0\n",
        "\n",
        "# Split data: 70% train, 15% validation, 15% test\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y_normalized, test_size=0.15, random_state=42\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1765, random_state=42  # 0.1765 * 0.85 ‚âà 0.15\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set: {len(X_train)} samples\")\n",
        "print(f\"Validation set: {len(X_val)} samples\")\n",
        "print(f\"Test set: {len(X_test)} samples\")\n",
        "\n",
        "# Standardize features (fit on train, transform all)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Save scaler parameters for inference\n",
        "scaler_params = {\n",
        "    'mean': scaler.mean_.tolist(),\n",
        "    'scale': scaler.scale_.tolist(),\n",
        "    'feature_names': feature_cols\n",
        "}\n",
        "\n",
        "with open('scaler_params.json', 'w') as f:\n",
        "    json.dump(scaler_params, f, indent=2)\n",
        "\n",
        "print(\"\\n‚úÖ Scaler parameters saved to scaler_params.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Define Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PronunciationScorer(nn.Module):\n",
        "    \"\"\"Neural network for pronunciation quality scoring.\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout=0.3):\n",
        "        super(PronunciationScorer, self).__init__()\n",
        "        \n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        \n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "        \n",
        "        # Output layer: single score [0, 1]\n",
        "        layers.append(nn.Linear(prev_dim, 1))\n",
        "        layers.append(nn.Sigmoid())  # Constrain output to [0, 1]\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "model = PronunciationScorer(input_dim=input_dim).to(device)\n",
        "\n",
        "print(f\"Model architecture:\")\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 100\n",
        "PATIENCE = 15  # Early stopping patience\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        ")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
        "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)\n",
        "X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
        "y_val_tensor = torch.FloatTensor(y_val).unsqueeze(1).to(device)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
        "y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1).to(device)\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "print(f\"Training setup complete\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Max epochs: {NUM_EPOCHS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèãÔ∏è Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'train_mae': [],\n",
        "    'val_mae': []\n",
        "}\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "best_model_state = None\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_mae = 0.0\n",
        "    \n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        train_mae += torch.mean(torch.abs(outputs - batch_y)).item()\n",
        "    \n",
        "    train_loss /= len(train_loader)\n",
        "    train_mae /= len(train_loader)\n",
        "    \n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        val_loss = criterion(val_outputs, y_val_tensor).item()\n",
        "        val_mae = torch.mean(torch.abs(val_outputs - y_val_tensor)).item()\n",
        "    \n",
        "    # Record history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['train_mae'].append(train_mae * 100)  # Convert to 0-100 scale\n",
        "    history['val_mae'].append(val_mae * 100)\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # Print progress\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}:\")\n",
        "        print(f\"  Train Loss: {train_loss:.6f}, MAE: {train_mae*100:.2f}\")\n",
        "        print(f\"  Val Loss: {val_loss:.6f}, MAE: {val_mae*100:.2f}\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_model_state = model.state_dict().copy()\n",
        "        print(f\"  ‚úì New best model (val_loss: {val_loss:.6f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(best_model_state)\n",
        "print(\"\\n‚úÖ Training complete! Best model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Visualize Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Loss plot\n",
        "axes[0].plot(history['train_loss'], label='Train Loss')\n",
        "axes[0].plot(history['val_loss'], label='Val Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('MSE Loss')\n",
        "axes[0].set_title('Training & Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# MAE plot\n",
        "axes[1].plot(history['train_mae'], label='Train MAE')\n",
        "axes[1].plot(history['val_mae'], label='Val MAE')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Mean Absolute Error (0-100 scale)')\n",
        "axes[1].set_title('Training & Validation MAE')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final Train MAE: {history['train_mae'][-1]:.2f}\")\n",
        "print(f\"Final Val MAE: {history['val_mae'][-1]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    test_loss = criterion(test_outputs, y_test_tensor).item()\n",
        "    test_mae = torch.mean(torch.abs(test_outputs - y_test_tensor)).item() * 100\n",
        "\n",
        "# Convert predictions to 0-100 scale\n",
        "y_test_pred = test_outputs.cpu().numpy() * 100\n",
        "y_test_actual = y_test * 100\n",
        "\n",
        "print(\"Test Set Performance:\")\n",
        "print(f\"  MSE Loss: {test_loss:.6f}\")\n",
        "print(f\"  MAE: {test_mae:.2f}\")\n",
        "print(f\"  RMSE: {np.sqrt(test_loss) * 100:.2f}\")\n",
        "\n",
        "# Scatter plot: predicted vs actual\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test_actual, y_test_pred, alpha=0.6)\n",
        "plt.plot([0, 100], [0, 100], 'r--', label='Perfect prediction')\n",
        "plt.xlabel('Actual Score')\n",
        "plt.ylabel('Predicted Score')\n",
        "plt.title('Test Set: Predicted vs Actual Scores')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Error distribution\n",
        "errors = y_test_pred.flatten() - y_test_actual\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(errors, bins=30, edgecolor='black')\n",
        "plt.xlabel('Prediction Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title(f'Error Distribution (Mean: {np.mean(errors):.2f}, Std: {np.std(errors):.2f})')\n",
        "plt.axvline(x=0, color='r', linestyle='--')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Export Model for Backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model weights\n",
        "torch.save(model.state_dict(), 'pronunciation_scorer.pth')\n",
        "\n",
        "# Save model architecture info\n",
        "model_info = {\n",
        "    'input_dim': input_dim,\n",
        "    'hidden_dims': [128, 64, 32],\n",
        "    'dropout': 0.3,\n",
        "    'test_mae': float(test_mae),\n",
        "    'test_rmse': float(np.sqrt(test_loss) * 100),\n",
        "    'num_samples': len(df),\n",
        "    'num_features': input_dim\n",
        "}\n",
        "\n",
        "with open('model_info.json', 'w') as f:\n",
        "    json.dump(model_info, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Model exported:\")\n",
        "print(\"   - pronunciation_scorer.pth (model weights)\")\n",
        "print(\"   - model_info.json (architecture)\")\n",
        "print(\"   - scaler_params.json (feature normalization)\")\n",
        "print(\"\\nDownload these 3 files to your backend/models/ directory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download files\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading model files...\")\n",
        "files.download('pronunciation_scorer.pth')\n",
        "files.download('model_info.json')\n",
        "files.download('scaler_params.json')\n",
        "print(\"‚úÖ Download complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Sample Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show some sample predictions\n",
        "model.eval()\n",
        "num_samples = 10\n",
        "\n",
        "with torch.no_grad():\n",
        "    sample_indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
        "    \n",
        "    print(\"Sample Predictions:\\n\")\n",
        "    print(f\"{'Actual':<10} {'Predicted':<10} {'Error':<10}\")\n",
        "    print(\"-\" * 35)\n",
        "    \n",
        "    for idx in sample_indices:\n",
        "        actual = y_test[idx] * 100\n",
        "        features = torch.FloatTensor(X_test_scaled[idx:idx+1]).to(device)\n",
        "        predicted = model(features).item() * 100\n",
        "        error = predicted - actual\n",
        "        \n",
        "        print(f\"{actual:<10.1f} {predicted:<10.1f} {error:+.1f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
